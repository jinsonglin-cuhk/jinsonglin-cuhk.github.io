<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html>
    <head>
    <meta name="google-site-verification" content="eoPCGBBxDIK0Ff9Dk_dXsuHMTNzzSEZMbsfO4zriBK8" />
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="keywords" content="Jinsong Lin, CUHK, HK ">
    <meta name="description" content="Jinsong Lin's home page">
<!--    <link href="main.css" media="all" rel="stylesheet">-->
    <link rel="stylesheet" href="jemdoc.css" type="text/css">
    <title>Jinsong (Jason) Lin</title>
    </head>

<body>


<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">
					<h1>Jinsong (Jason) Lin  &nbsp &nbsp  林劲松</h1>
<!--					<h1>林劲松</h1><h1>-->
				</div>

				<h3>1st Year Phd Student @ CUHK <br><br>
<!--					Visiting Ph.D. Student @ Stanford University <br><br>-->
<!--				Student Researcher @ Google Research <br><br>-->
<!--				Amazon Fellow with Amazon ML Fellowship (2022-2023)-->
				</h3>
<!--				<h3>Student Researcher @ Google Research</h3>-->
<!--				<h3>Amazon Fellow with Amazon ML Fellowship (2022-2023)</h3>-->
<!--                <h3>University of Southern California</h3>-->
				<p>
<!--					Department of Computer Science @-->
<!--					University of Southern California <br>-->
<!--					Rm B06, Hedco Neurosciences Building, 3641 Watt Way, Los Angeles, CA 90089-2520, USA <br>-->

					Email: jslin@link.cuhk.edu.hk

				</p>
				<p> <a href="https://scholar.google.com/citations?user=IT5CsMYAAAAJ&hl=zh-CN"><img src="./pics/google_scholar3.png" height="40px" style="margin-bottom:-3px"></a>
                    <a href="https://https://www.linkedin.com/in/jinsong-lin-13a1bb387/"><img src="./pics/LinkedIn2.png" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://x.com/song_jin55802"><img src="./pics/X.png" height="40px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/jinsonglin-cuhk"><img src="./pics/github_s.jpg" height="40px" style="margin-bottom:-3px"></a>
                    <!-- <a href="files/CV_YunhaoGe.pdf"><img src="./pics/cv2.png" height="40px" style="margin-bottom:-3px"></a> -->
					&nbsp &nbsp
					<a href="#C1">[<font size="3" color="#CB4335"><b>About Me</b></font>] </a>
					<a href="#C2">[<font size="3" color="#CB4335"><b>News</b></font>]</a>
					<a href="#C3">[<font size="3" color="#CB4335"><b>Publications</b></font>]</a>
					<a href="#C4">[<font size="3" color="#CB4335"><b>Experience</b></font>]</a>
<!--					<a href="#C5">[<font size="3" color="#CB4335"><b>Mentorship</b></font>]</a>-->
					</li>
				</p>
			</td>
			<td>
				<img src="pics/Jason.jpg" border="0" width="200"><br>
<!--				<img src="pics/Yunhao Ge3.jpg" border="0" width="230"><br>-->
<!--				<img src="pics/cover.png" border="0" width="540"><br>-->
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<style>-->
<!--ul-->
<!--{-->
<!--	list-style-type:none;-->
<!--	margin:0;-->
<!--	padding:0;-->
<!--}-->
<!--li a:hover {-->
<!--    background-color: #555;-->
<!--    color: white;-->
<!--}-->

<!--</style>-->


<!--   #0F73B6-->

<h2><a id="C1" ><font color="#CB4335">About Me</font></a></h2>
<p>
	I am a Research Scientist at <a  href="https://research.nvidia.com/labs/gear/">NVIDIA's Generalist Embodied Agent Research</a>, working on <a  href="https://developer.nvidia.com/isaac/gr00t/">Project Groot</a>. I have broad research interests in Multimodal Foundation model and Robotics, with a recent focus on building generally capable agents based on world foundation models. 
I was previously a Research Scientist at <a  href="https://research.nvidia.com/labs/dir/">NVIDIA's Deep Imagination Research</a>, where I worked on <a  href="https://www.nvidia.com/en-us/ai/cosmos/">NVIDIA Cosmos</a>. I received my Ph.D. in
 Computer Science from <a  href="https://www.cs.usc.edu/">University of Southern California</a> advised by Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>,
and was honored with the <a  target="_blank" href=https://www.amazon.science/latest-news/amazon-and-usc-name-three-new-ml-fellows target="_blank" rel="external"> Amazon ML Fellowship</a>.
I was a Visiting Ph.D. Student at <a  href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a> advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>.
<!-- <p>
		I am a Research Scientist at <a  href="https://research.nvidia.com/labs/dir/">NVIDIA's Deep Imagination Research group</a>. I have broad research interests in Computer Vision and Robotics, with a recent focus on building multimodal foundation models for physical AI. 
	My research has been integrated into several products for NVIDIA, including <a  href="https://www.nvidia.com/en-us/ai/cosmos/">NVIDIA Cosmos</a> and <a  href="https://www.nvidia.com/en-us/gpu-cloud/edify/">NVIDIA Edify</a>. I received my Ph.D. in
	 Computer Science from <a  href="https://www.cs.usc.edu/">University of Southern California</a> advised by Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>,
	and was honored with the <a  target="_blank" href=https://www.amazon.science/latest-news/amazon-and-usc-name-three-new-ml-fellows target="_blank" rel="external"> Amazon ML Fellowship</a>.
	I was a Visiting Ph.D. Student at <a  href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a> advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>. -->
<!--	I am a Ph.D. Candidate in the CS Department at <a  href="https://www.cs.usc.edu/">University of Southern California</a>,-->
<!--	 advised by Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>.-->
<!--	I am also a Visiting PhD Student at <a  href="https://svl.stanford.edu/">Stanford Vision and Learning Lab (SVL)</a> advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>.-->

	<!-- I have broad research interests in Computer Vision and Robotics, with a recent focus on building multimodal foundation models for physical AI. -->
<!--	My primary research interest lies in controllable data generation, intending to use generated dataset train AI models that can effectively perceive, understand, interact with, and reason about the physical world.-->
<!--	I'm interested in learning controllable data generation to train AI models to perceive, understand, interact, and reason the physical world. My current research focuses include:-->
<!--		I'm interested in how could human efficiently teach AI to learn the human ability to perceive, understand, interact, and reason the physical world. My current research focuses include:-->
<!--	I'm interested in Machine Learning, Computer vision, and their applications towards Trustworthy, Human-like AI and Data-centric AI. My current research focuses include:-->
<ul>
<!--		<li>Controllable Data Generation: <i>[Learning to generate]</i> Use generative models and neural renderers to synthesize realistic and physically plausible data automatically. <i>[Generating to learn]</i> AI models trained with synthetic data can solve real-world Vision and Robotics tasks.</li>-->
<!--       <li>Vision Language Models, Lifelong Learning, and Reliable Deep Learning </li>-->
	<!--	<li>Large Multimodal Models (LMM), Large Language Models(LLM), Lifelong Learning, and Reliable Deep Learning </li>-->
	<!--	<li>Vision‑Language Models, Lifelong Learning, Visual Reasoning and Reliable Deep Learning </li>-->
<!--	<li>Controllable Data Generation: using generative models and neural renderer to automatically synthesize realistic and physical plausible data to train AI models to solve real-world Computer Vision and Robotics problems</li>-->
<!--	<li>Vision‑Language Models, Lifelong Learning, Visual Reasoning and Reliable Deep Learning </li>-->
<!--	<li>Human-inspired Learning Algorithm (Lifelong Learning,  Multi-modal Models, Visual Reasoning) </li>-->

<!--	<li>Learning from Synthetic Data (Sim2Real): using neural renderer (NeRF, Stable Diffusion) and simulation to synthesize realistic and physical plausible data to solve real-world Computer Vision and Robotics problems-->
<!--		   with minimal human supervision </li>-->
<!--	<li>Reliable Deep Learning (Robustness, Out-of-distribution (OOD) Detection, Interpretability)</li>-->
<!--		<li>Reliable Deep Learning (Robustness, Out-of-distribution (OOD) Detection, Interpretability)</li>-->
<!--	  <li>Synthetic data for Vision/Robotics (Sim2Real): using neural renderer (NeRF, DALL-E / Stable Diffusion, GAN) to synthesize realistic and physical plausible data to solve real-world Computer Vision and Robotics problems-->
<!--		   with minimal supervision </li>-->
<!--	  <li>Human-inspired Learning Algorithm (Continual Learning,  Multi-modal Models, Visual Reasoning) </li>-->
<!--	  <li>Trustworthy AI (Interpretability, Robustness, Out-of-distribution (OOD) Detection, Human-to-AI Knowledge Exchange, Causality)</li>-->
<!--	  <li>Data-centric AI (Sim2Real): using neural renderer (NeRF, DALL-E / Stable Diffusion, GAN) to synthesize realistic and physical plausible data to solve real-world CV problems-->
<!--		  (classification, detection, segmentation) with minimal supervision </li>-->
<!--	  <li>Human-like AI to simulate human cognitive learning ability (Continual Learning,  Multi-modal Models (CLIP), Visual Reasoning) </li>-->

<!--		I'm interested in Machine Learning, Computer vision, and their applications towards Human-centric / Humanoid AI and Data-centric AI. My current research focuses include:-->
<!--<ul>-->
<!--	  <li>Human-centric properties of AI models (Causal Explainability, Robustness, Domain Adaptation, Out-of-distribution Detection (OOD), Human-to-AI Knowledge Exchange)</li>-->
<!--	  <li>Simulate human cognitive learning ability (Continual Learning,  Multi-modal (CLIP), Imagination, Reasoning, Visual Recognition) </li>-->
<!--	  <li>Data-centric AI: using synthetic data and neural renderer (NeRF, DALL-E, GAN, VAE) to solve real-world computer vision problems-->
<!--		  (classification, detection, segmentation) with minimal supervision </li>-->
<!--<p>I am now a Ph.D. Candidate in the Department of CS at <a  href="https://www.cs.usc.edu/">University of Southern California</a> (USC) and a member of <a  href="http://ilab.usc.edu/">iLab</a>,-->
<!--   working with Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=xhUvqK8AAAAJ&hl=en target="_blank" rel="external">Laurent Itti</a>.-->
<!--		I'm interested in Machine Learning, Computer vision, and their applications towards Human-centric / Humanoid AI and Data-centric AI. My current research focuses include:-->
<!--<ul>-->
<!--	  <li>Human-centric properties of AI models (Causal Explainable AI, Human-to-AI / AI-to-AI Knowledge Exchange, Domain Adaptation, Out-of-distribution Detection (OOD))</li>-->
<!--	  <li>Simulate human cognitive learning ability (Continual Learning, Imagination, Reasoning, Visual Recognition) </li>-->
<!--	  <li>How generative models (NeRF, GAN, VAE) and multi-modal models (DALL-E, CLIP) help downstream discriminative models (detection, segmentation) </li>-->
<!--	  <li>Causal Explainable AI ((1) Reveal reasoning logic and causality of Neural Networks (NN) (2) Use explanation / causality as feedback to help improve the performance of the original NN.) </li>-->
	<!--	  <li>Understanding AI models beyond accuracy (disentangled representation learning, human-NN knowledge exchange, steerability, generalization, domain adaptation, and bias)</li>-->
	<!--	  <li>Humanoid Neural Network (simulating human cognitive learning ability (Imagination, Reasoning, Visual Recognition, Continual Learning) by using various learning algorithms (Generative models, Representation Learning, Graph Neural Network, etc.)</li>-->
<!--	  <li>Effortless AI (how generative models (NeRF, DALL-E, GAN, VAE) reduce human effort and boost discriminative models)</li>-->
	</ul>

<!--	I also work closely with-->
<!--	Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a> (<a  target="_blank" href=https://www.microsoft.com/en-us/research/ target="_blank" rel="external">Microsoft Research</a>),-->
<!--	Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a> (<a  target="_blank" href=https://research.google/teams/brain/ target="_blank" rel="external">Google Brain</a>),-->
<!--	Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a> (<a  target="_blank" href=https://research.google/ target="_blank" rel="external">Google Research</a>),-->
<!--	and Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external">Ziyan Wu</a> (<a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">UII America</a>).-->


	Previously, I was fortunate to intern/work at <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research</a>, <a  target="_blank" href=https://cloud.google.com/ target="_blank" rel="external"> Google Cloud AI</a>,
<a  target="_blank" href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13562&sort_by=most-recent" target="_blank" rel="external">Microsoft Research</a>,
<a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">United Imaging Intelligence</a>,
	and <a  target="_blank" href="http://flexiv.com/" target="_blank" rel="external">Flexiv Robotics</a>. Before that, I got my M.Sc. degree at Robotics Institute at <a  target="_blank" href="https://www.tsinghua.edu.cn/en/index.htm">Tsinghua University</a>.


</p>

<p> &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp &nbsp
	<a href="https://cuhk.edu.hk/english/"><img src="./pics/cuhk_logo.jpg" height="120px" style="margin-bottom:-9px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.cbicr.tsinghua.edu.cn/en/"><img src="./pics/cuicr.jpg" height="120px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="https://www.tsinghua.edu.cn/en/index.htm"><img src="./pics/Tsinghua.webp" height="120px" style="margin-bottom:-3px"></a>&nbsp &nbsp &nbsp
	<a href="http://www.en.sdu.edu.cn/"><img src="./pics/sdu_logo.jpg" height="125px" style="margin-bottom:-3px"></a>
</p>


<!--	Previously, I was a student researcher at <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research </a> working with-->
<!--	Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a>,-->
<!--	Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a>,-->
<!--	Dr. <a  target="_blank" href=http://www.gatsby.ucl.ac.uk/~balaji/ target="_blank" rel="external"> Balaji Lakshminarayanan </a>-->
<!--    and   Prof. <a  target="_blank" href=http://faculty.ucmerced.edu/mhyang/ target="_blank" rel="external"> Ming-Hsuan Yang</a>; a student researcher at <a  target="_blank" href=https://cloud.google.com/ target="_blank" rel="external"> Google Cloud AI </a> working with Dr. <a  target="_blank" href=https://sercanarik.com/ rel="external"> Sercan Arik </a>-->
<!--        and Dr. <a  target="_blank" href=https://sites.google.com/view/jinsungyoon target="_blank" rel="external"> Jinsung Yoon </a>; a research intern at-->
<!--<a  target="_blank" href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13562&sort_by=most-recent" target="_blank" rel="external">Microsoft Research</a></li>-->
<!--		  working with Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a> and-->
<!--and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a>; and a research intern at-->
<!-- <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">UII America</a></li> working with-->
<!--        Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>-->
<!--            and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a>. <br>-->
<!--</p>-->
<!--	Before that, I got my M.Sc. degree at Robotics Institute at <a  target="_blank" href="http://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>.-->
<!--	and B.Eng. degree of Mechatronics at <a  target="_blank" href="http://www.en.sdu.edu.cn/"> Shandong University</a>. </p>-->
<!-- I have been lucky to work with Prof. <a  target="_blank" href=https://scholar.google.com/citations?user=v6VYQC8AAAAJ&hl=en target="_blank" rel="external"> Dinggang Shen </a> and-->
<!--	Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=INL-unYAAAAJ&hl=en target="_blank" rel="external"> Shu Liao </a> during my internship at <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">United Imaging Intelligence</a>,-->
<!--	as well as Prof. <a  target="_blank" href=https://www.mvig.org/ target="_blank" rel="external"> Cewu Lu </a> and-->
<!--	Dr. <a  target="_blank" href=https://www.linkedin.com/in/shuyun-chung-67b5337a/ target="_blank" rel="external"> Shuyun Chong</a> when I was a Computer Vision Engineer at <a  target="_blank" href="http://flexiv.com/" target="_blank" rel="external">Flexiv Robotics</a>.-->
<!--</p>-->

<!--<ul>-->
<!--	  <li>Causal Explainable AI ((1) Understanding reasoning logic and causality of Neural Networks (NN) (2) Use explanation as feedback to help improve the performance of the original NN.) </li>-->
<!--	  <li>Interpretable human-AI interaction (understanding AI models beyond accuracy, such as disentangled representation learning, human-NN knowledge exchange, steerability, generalization, fairness and bias)</li>-->
<!--	  <li>Humanoid Neural Network (simulating human cognitive learning ability (Imagination, Reasoning, Visual Recognition) by using various learning algorithms (Generative models, Representation Learning, Graph Neural Network, Contrastive Learning, etc.)</li>-->
<!--	  <li>Effortless AI (how generative models reduce human effort and boost discriminative models)</li>-->
<!--	</ul>-->

<!--	I'm interested in Machine Learning, Computer vision, and their applications towards Artificial General Intelligence (AGI). My current research focuses include:-->
<!--<ul>-->
<!--	  <li>interpretable human-AI interaction (interpretability, steerability, disentangled representation learning)</li>-->
<!--	  <li>generative models (data augmentation, how generative models boost discriminative models)</li>-->
<!--	  <li>graph neural networks (structure and relationship learning)</li>-->
<!--	  <li>visual reasoning, attention and saliency (cognitive learning, eye tracking)</li>-->
<!--	</ul>-->
<!--<p>My research interests lie in Machine Learning, Computer vision, and AGI. Currently, I am focusing on simulating Cognitive Baby Learning (Imagination, Reasoning, Attention)-->
<!--by using various learning algorithms (Representation Learning, Generative models, GNN, Reinforcement Learning,  Meta-Learning, etc.).</p>-->
<!--<p> <b>Research opportunities</b>: I am happy to collaboration. If you are interested, please send me an email.-->
<!--	I especially encourage USC master/undergraduate students who want to involve exciting projects targeting top-tier conferences/journals to reach out.-->
<!--</p>-->
<!--I am happy to collaborate and/or answer questions about my research.-->
<!--<p>My research interests lie in Computer vision, Robotics and General AI. Currently, I am-->
<!--focusing on simulating baby learning (Reasoning, Attention, Imagination) by using various learning algorithms (Representation Learning, Adversarial Learning,-->
<!--Meta Learning, GNN, Reinforcement Learning, etc.).</p>-->
<br>




<h2><a id="C2" ><font color="#CB4335">News & Updates</font></a></h2>
<ul>

<div style="height:300px;width:fit-content;overflow:auto;background:#FFFFFF;">

	<li>
		<p>[2025/12/15] We have released <a  target="_blank" href="https://research.nvidia.com/labs/gear/gr00t-n1_6/">Groot N1.6</a>, a open vision-language-action (VLA) model for generalized humanoid robot skills!
		    </p>
	<li>
		<p>[2025/12/15] We have released <a  target="_blank" href="https://luling06.github.io/I-Scene-project/">I-Scene</a>: 3D Instance Models are Implicit Generalizable Spatial Learners!
		    </p>
	<li>
		<p>[2025/10/28] We have released <a  target="_blank" href="https://research.nvidia.com/labs/dir/cosmos-predict2.5/">Cosmos Predict2.5 and Cosmos-Transfer2.5</a>, an improved world foundation model platform for physical AI!
			</p>
	<li>
		<p>[2025/03/18] We have released <a  target="_blank" href="https://research.nvidia.com/labs/dir/cosmos-transfer1/">Cosmos Transfer1</a>, a world-to-world transfer model designed to bridge the perceptual divide between simulated and real-world environments!
		    </p>
	<li>
		<p>[2025/02/07] One paper, <a  target="_blank" href="https://briannlongzhao.github.io/DreamDistribution/">DreamDistribution</a> for generating diverse customized images, is accepted by <b>ICLR 2025</b>. Code released.
		    </p>
	<li>
		<p>[2025/01/06] We have released <a  target="_blank" href="https://www.nvidia.com/en-us/ai/cosmos/">Cosmos</a>, a world foundation model platform for physical AI!
		    </p>
	<li>
		<p>[2024/11/11] We have released <a  target="_blank" href="https://arxiv.org/abs/2411.07135">Edify 3D</a>, a cutting-edge technology for 3D generation!
		    </p>
	<li>
		<p>[2024/11/11] We have released <a  target="_blank" href="https://arxiv.org/abs/2411.07126">Edify Image</a>, a family of diffusion models for photorealistic image generation!
		    </p>
	<li>	
		<p>[2024/7/30] <a  target="_blank" href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&linkId=100000277255797">GenUSD</a> is proudly featured in the <a  target="_blank" href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&linkId=100000277255797">Real-Time Live! at SIGGRAPH 2024</a> .
		    </p>
	<li>
		<p>[2024/5/15] One paper, <a  target="_blank" href="https://behavior-vision-suite.github.io/">BEHAVIOR Vision Suite</a> for Customizable Dataset Generation, is accepted by <b>CVPR 2024 (Highlight)</b>. Code released.
		    </p>
	<li>	
		<p>[2024/5/1] One paper, <a  target="_blank" href="https://research.nvidia.com/labs/dir/vfc/">Visual Fact Checker</a>, LLM Agent Tool Use for 2D/3D captioning, is accepted by <b>CVPR 2024</b>.
			</p>
	<li>
		<p>[2023/12/21] We release the paper and code of <a  target="_blank" href="https://briannlongzhao.github.io/DreamDistribution/">DreamDistribution </a> for personalized 2D/3D generation.
			</p>
	<li>
		<p>[2023/12/18] Starting a new journey at <a  target="_blank" href="https://research.nvidia.com/labs/dir/">NVIDIA Research </a> as a Research Scientist.
			</p>
	<li>
		<p>[2023/09/23] One paper on <b>3D Copy-Paste</b> is accepted by <b>NeurIPS 2023</b>.
			</p>
	<li>
		<p>[2023/07/26] One paper on <b>Lifelong (Continual) Learning</b> is accepted by <b>ICCV 2023</b>.
			</p>
	<li>
		<p>[2023/05/09] One paper on <b>Shared Knowledge Lifelong Learning</b>, a new Lifelong Learning paradigm, is accepted by <b>TMLR</b>.
			</p>
	<li>
		<p>[2023/02/27] One paper on <b>Multi-modal models' Robustness and Generalization</b> is accepted by <b>CVPR</b> 2023.
			</p>
	</li>

	<li>
		<p>[2022/12/01] Starting a new journey at <a  target="_blank" href="https://svl.stanford.edu/">Stanford Vision and Learning (SVL) Lab </a> as a Visiting Student Researcher, advised by Prof. <a  target="_blank" href=https://jiajunwu.com/ target="_blank" rel="external">Jiajun Wu</a>.
			</p>
	</li>
	<li>
		<p>[2022/08/16] I was awarded the Amazon ML Fellowship (2022-2023), and will be an Amazon Fellow at
			<a  target="_blank" href=https://trustedai.usc.edu/ target="_blank" rel="external"> USC + Amazon Center on Secure & Trusted Machine Learning</a>. Thank you Amazon! </p>
	</li>
	<li>
		<p>[2022/08/16] One paper on <b>Disentangled and Convex Representation learning</b> is accepted by <b>WACV</b> 2023, code is coming soon.</p>
	</li>
	<li>
		<p>[2022/07/03] Two papers on <b>NeRF</b> and <b>Humanoid Neural Network</b> are accepted by <b>ECCV</b> 2022, code are released.</p>
	</li>
	<li>
		<p>[2022/05/31] I will be joining  <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research </a>  as a student researcher, advised by
			Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a>,
	Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a>,
	Dr. <a  target="_blank" href=http://www.gatsby.ucl.ac.uk/~balaji/ target="_blank" rel="external"> Balaji Lakshminarayanan </a>
    and   Prof. <a  target="_blank" href=http://faculty.ucmerced.edu/mhyang/ target="_blank" rel="external"> Ming-Hsuan Yang. </a></li></p>
	</li>
	<li>
		<p>[2022/01/20] Finally passed my qual exam and officially became a PhD <b>Candidate</b> now.</p>
	</li>
	<li>
		<p>[2021/08/23] I will be joining  <a  target="_blank" href=https://cloud.google.com/ target="_blank" rel="external"> Google Cloud AI </a>  as a student researcher, advised by
        Dr. <a  target="_blank" href=https://sercanarik.com/ rel="external"> Sercan Arik </a>
        and Dr. <a  target="_blank" href=https://sites.google.com/view/jinsungyoon target="_blank" rel="external"> Jinsung Yoon </a></li></p>
	</li>
	<li>
		<p>[2021/07/15] <a  target="_blank" href=https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/ target="_blank" rel="external"> USC News </a>,
			<a  target="_blank" href=https://techxplore.com/news/2021-07-enabling-artificial-intelligence.html target="_blank" rel="external"> Tech Xplore </a>,
			<a  target="_blank" href=https://www.technologynetworks.com/informatics/news/enabling-ai-to-use-its-imagination-350886 target="_blank" rel="external"> Technology Networks </a>
			and other media pressed our <b>ICLR</b> 2021 paper: <a href="https://arxiv.org/pdf/2009.06586.pdf" target="_blank">Group-Supervised Learning</a> (Enabling the 'imagination' of artificial intelligence)</p>
	</li>
	<li>
		<p>[2021/05/17] I will be joining Computer Vision Group at <a  target="_blank" href=https://www.microsoft.com/en-us/research/ target="_blank" rel="external"> Microsoft Research </a>  Redmond as a research intern in summer 2021, advised by
        Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a>
        and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a></li></p>
	</li>
<!--	<li>-->
<!--		<p>[2021/05/08] Serving as a reviewer for NeurIPS 2021 and ICLR 2022!</p>-->
<!--	</li>-->
	<li>
		<p>[2021/04/07] Releasing <a  target="_blank" href=http://ilab.usc.edu/datasets/i2sg target="_blank" rel="external">Img2SceneGraph</a>,
		a pipeline that transfers images to scene graphs with node attributes!
		Welcome to <a  target="_blank" href=http://ilab.usc.edu/datasets/i2sg target="_blank" rel="external"> Download </a> and try!</p>
	</li>
	<li>
		<p>[2021/04/02] One paper (Graph Autoencoder for Graph Compression and Representation Learning)
		was accepted by Neural Compression Workshop @<b>ICLR</b> 2021 as <strong style="color:blue">Spotlight</strong>!</p>
	</li>
	<li>
		<p>[2021/02/28] One paper (A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts)
		was accepted by <b>CVPR</b> 2021!</p>
	</li>
	<li>
		<p>[2021/01/16] One paper (Beneficial Perturbation Network for designing general adaptive artificial intelligence systems)
		was accepted by <b>TNNLS</b>!</p>
	</li>
	<li>
		<p>[2021/01/12] One paper (Zero-shot Synthesis with Group-Supervised Learning) was accepted by <b>ICLR</b> 2021!</p>
	</li>
	<li>
		<p>[2020/09/14] <a  target="_blank" href=http://ilab.usc.edu/datasets/fonts target="_blank" rel="external"> Fonts dataset </a> was proposed for fast testing and idea iteration on disentangled representation learning and zero-shot synthesis.
			Welcome to <a  target="_blank" href=http://ilab.usc.edu/datasets/fonts target="_blank" rel="external"> Download </a> and try!</p>
	</li>
	<li>
		<p>[2020/07/02] One paper (Pose Augmentation: Class-agnostic Object Pose Transformation) was accepted by <b>ECCV</b> 2020!</p>
	</li>
	<li>
		<p>[2020/05/12] I will be joining UII America as a research intern in summer 2020, advised by
        Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>
        and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a></li></p>
	</li>
	<li>
		<p>[2019/08/12] I will be joining USC CS Ph.D. Program in fall 2019, advised by
        Prof. <a  target="_blank" href=http://ilab.usc.edu/itti/ target="_blank" rel="external">Laurent Itti</a>.</p>
	</li>
	<li>
		<p>[2019/07/01] One paper (Synthesis and inpainting-based MR-CT registration) was accepted by <b>MICCAI</b> 2019.</p>
	</li>
	<li>
		<p>[2019/03/01] One paper (Unpaired Whole-Body Mr to CT Synthesis) was accepted by <b>ISBI</b> 2019.</p>
	</li>

</div>
</ul>
<br>
<!--<h2>Education</h2>-->
<!--<hr>-->
<!--  <img id="school_logo" src="./pics/USC_logo.png">-->
<!--  <h4> University of Southern University, Los Angeles, USA (Aug. 2019 - present)</h4>-->
<!--  <ul>-->
<!--	<li>-->
<!--	  <b>PhD of Computer Science</b>, ilab @ Computer Science Department</li>-->
<!--	<li>Major Orientation: Deep Learning and Reinforcement learning for object recognition <br />Baby learning inspired causal inference </li>-->
<!--	<li>Annenberg Graduate Fellowship at University of Southern California </li>-->
<!--  </ul>-->
<!--  -->
<!--  <img id="school_logo" src="./pics/sjtu_logo.png">-->
<!--  <h4> Shanghai Jiao Tong University, Shanghai, China (Sep. 2016 - Jun. 2019)</h4>-->
<!--  <ul>-->
<!--	<li>-->
<!--	  <b>Master of Science(MSc)</b>, ROBOTICS AND INTELLIGENCE GROUP</li>-->
<!--	<li>Major Orientation: Deep Learning for Medical Image Computing(Computer Vison)<br />deep learning for Robotics and Machine Vision </li>-->
<!--	<li>Overall performance ranked: 6/210 </li>-->
<!--	<li>Excellent master's thesis "Automatic focusing and global precise imaging of <br>pathological microscope based on convolutional neural network "</li>-->
<!--  </ul>-->
<!--  -->
<!--  <img id="school_logo" src="./pics/sdu_logo.jpg">-->
<!--  <h4> Shandong University, Jinan, Shandong, China (Sep. 2012 - Jun. 2016)</h4>-->
<!--  <ul>-->
<!--	<li>-->
<!--	  <b>Bachelor of Engineering(B.Eng)</b>, MECHATRONICS </li>-->
<!--	<li>Major Orientation: Robotics Intelligent Control and Bionic Robotics</li>-->
<!--	<li>Overall performance ranked:-->
<!--	  <strong style="color:black">1/66</strong>   GPA: Overall: 86.08/100 | Major: 93.38/100 </li>-->
<!--  </ul>-->

<!--<h2><a id="C3" ><font color="#CB4335">Preprints</font></a></h2>-->

<!--	<table id="tbPreprints" width="100%">-->
<!--		-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--		<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/EM-paste.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>EM-Paste: EM-guided Cut-Paste with DALL-E Augmentation for Image-level Weakly Supervised Instance Segmentation</b><br>-->
<!--<br>-->
<!--&lt;!&ndash;		<br>&ndash;&gt;-->
<!--&lt;!&ndash;			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge</b><sup>*</sup>, Jiashu Xu<sup>*</sup>, Brian Nlong Zhao, Laurent Itti, Vibhav Vineet (*=equal contribution) <br>-->
<!--		<em>arXiv:2212.07629, 2022.</em>-->
<!--		<p></p>-->
<!--			<p>[<a href="https://arxiv.org/pdf/2212.07629.pdf" target="_blank">paper</a>]-->
<!--&lt;!&ndash;			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]&ndash;&gt;-->
<!--			</p>-->
<!--		</td>-->
<!--	</tr>-->

		



</table>

<h2><font color="#CB4335">Research Highlights</font></h2>

<div style="display: flex; justify-content: space-between; gap: 20px; margin: 30px 0; align-items: flex-start;">
	<!-- Panel 1: Multimodal Foundation Model for Physical AI -->
	<div style="flex: 1; padding: 15px; background-color: #f9f9f9; text-align: left;">
		<h3 style="color: rgb(0, 0, 0); margin: 10px 0 20px 0; font-size: 16px; font-weight: bold;">Generally Capable Agents based on World Foundation Models</h3>
		<!-- <p style="margin: 5px 0; font-size: 13px; color: #666;">World models for physical AI</p>
		<p style="margin: 5px 0; font-size: 11px; color: #888;"><strong style="color:rgb(20, 115, 204)">Best AI + Best overall of CES 2025</strong></p> -->
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			<!-- <strong>Pre-training:</strong>  -->
			<a target="_blank" href="https://research.nvidia.com/labs/gear/gr00t-n1_6/" style="color:rgb(20, 115, 204)">GR00T N1.6 </a><br>

			<!-- <strong>Post-training:</strong> 
			<a target="_blank" href="https://research.nvidia.com/labs/dir/cosmos-transfer1/" style="color:rgb(20, 115, 204)">Cosmos-Transfer1 </a> -->
		</p>
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			&nbsp;<br>&nbsp;
		</p>
		<video width="100%" height="180" controls autoplay loop muted style="margin-bottom: 10px;">
			<source src="pics/grootn1p6.mp4" type="video/mp4">
			Your browser does not support the video tag.
		</video>
		<!-- <h4 style="color: rgb(20, 115, 204); margin: 10px 0; font-size: 16px;">Cosmos</h4> -->
	</div>


	<!-- Panel 2: Multimodal Foundation Model for Physical AI -->
	<div style="flex: 1; padding: 15px; background-color: #f9f9f9; text-align: left;">
		<h3 style="color: rgb(0, 0, 0); margin: 10px 0 20px 0; font-size: 16px; font-weight: bold;">Multimodal Foundation Model for Physical AI</h3>
		<!-- <p style="margin: 5px 0; font-size: 13px; color: #666;">World models for physical AI</p>
		<p style="margin: 5px 0; font-size: 11px; color: #888;"><strong style="color:rgb(20, 115, 204)">Best AI + Best overall of CES 2025</strong></p> -->
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			<strong>Pre-training:</strong> 
			<a target="_blank" href="https://github.com/nvidia-cosmos/cosmos-predict2" style="color:rgb(20, 115, 204)">Cosmos Predict2 </a>  | 
			<a target="_blank" href="https://www.nvidia.com/en-us/ai/cosmos/" style="color:rgb(20, 115, 204)">Cosmos-Predict1 </a><br>

			<strong>Post-training:</strong> 
			<a target="_blank" href="https://research.nvidia.com/labs/dir/cosmos-transfer1/" style="color:rgb(20, 115, 204)">Cosmos-Transfer1 </a>
		</p>
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			&nbsp;<br>&nbsp;
		</p>
		<video width="100%" height="180" controls autoplay loop muted style="margin-bottom: 10px;">
			<source src="pics/Cosmos.mp4" type="video/mp4">
			Your browser does not support the video tag.
		</video>
		<!-- <h4 style="color: rgb(20, 115, 204); margin: 10px 0; font-size: 16px;">Cosmos</h4> -->
	</div>

	<!-- Panel 3: Physical Scene Generation and Understanding -->
	<div style="flex: 1; padding: 15px; background-color: #f9f9f9; text-align: left;">
		<h3 style="color: rgb(0, 0, 0); margin: 10px 0 20px 0; font-size: 16px; font-weight: bold;">Physical Scene Generation & Understanding</h3>
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			<strong>Generation:</strong> 
			<a target="_blank" href="https://luling06.github.io/I-Scene-project/" style="color:rgb(20, 115, 204)">I-Scene </a>  | 
			<a target="_blank" href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&linkId=100000277255797" style="color:rgb(20, 115, 204)">GenUSD </a>  | 
			<a target="_blank" href="https://arxiv.org/abs/2411.07135" style="color:rgb(20, 115, 204)">Edify3D </a> | 
			<a target="_blank" href="https://arxiv.org/abs/2411.07126" style="color:rgb(20, 115, 204)">Edify Image </a> | 
			<a target="_blank" href="https://behavior-vision-suite.github.io/" style="color:rgb(20, 115, 204)">BEHAVIOR Vision Suite </a> | 
			<a target="_blank" href="https://arxiv.org/abs/2312.05277" style="color:rgb(20, 115, 204)">3D Copy Paste </a> | 
			<a target="_blank" href="https://research.nvidia.com/labs/dir/scenethesis/" style="color:rgb(20, 115, 204)">Scenethesis </a> | 
			<a target="_blank" href="https://arxiv.org/abs/2506.00742" style="color:rgb(20, 115, 204)">ArtiScene </a><br>

			<strong>Understanding:</strong> 
			<a target="_blank" href="https://arxiv.org/abs/2404.19752" style="color:rgb(20, 115, 204)">Visual Fact Checker </a> | 
			<a target="_blank" href="https://describe-anything.github.io/" style="color:rgb(20, 115, 204)">Describe Anything </a>
		<video width="100%" height="180" controls autoplay loop muted style="margin-bottom: 10px;">
			<source src="pics/edify-3d.mp4" type="video/mp4">
			Your browser does not support the video tag.
		</video>
	</div>

	<!-- Panel 3: Controllable Generation -->
	<!-- <div style="flex: 1; padding: 15px; background-color: #f9f9f9; text-align: left;">
		<h3 style="color: rgb(0, 0, 0); margin: 10px 0 20px 0; font-size: 16px; font-weight: bold;">Controllable Generation</h3>
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			<a target="_blank" href="https://briannlongzhao.github.io/DreamDistribution/" style="color:rgb(20, 115, 204)">DreamDistribution </a> | 
			<a target="_blank" href="https://arxiv.org/abs/2411.07126" style="color:rgb(20, 115, 204)">Neural-Sim </a> | 
			<a target="_blank" href="https://arxiv.org/abs/2405.09546" style="color:rgb(20, 115, 204)">Group-Supervised Learning </a> | 
			<a target="_blank" href="https://arxiv.org/pdf/2206.09592.pdf" style="color:rgb(20, 115, 204)">DALL-E for Detection </a> | 
			<a target="_blank" href="https://arxiv.org/abs/2003.08526" style="color:rgb(20, 115, 204)">Pose Augmentation </a> 
		</p>
		<p style="margin: 10px 0 5px 0; font-size: 12px; color: #555;">
			&nbsp;<br>&nbsp;
		</p>
			<video width="100%" height="180" controls autoplay loop muted style="margin-bottom: 10px;">
			<source src="pics/dreamdistribution.mp4" type="video/mp4">
			Your browser does not support the video tag.
		</video>
	</div> -->
</div>

<h2><a id="C3" ><font color="#CB4335">Selected Publications</font></a> [<a href="https://scholar.google.com/citations?hl=en&user=QhjGr4oAAAAJ=en&user=QhjGr4oAAAAJ" style="color:rgb(20, 115, 204)">Google Scholar</a>]</h2>

<table id="tbPublications" width="100%">

	<tr>
		<td width="306">
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/grootn1p6.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>GR00T N1.6: An Improved Open Foundation Model for Generalist Humanoid Robots</b><br>
			<br>NVIDIA (<b>Yunhao Ge</b>: core contributor)<br>
		<p></p>
			<p>[<a href="https://research.nvidia.com/labs/gear/gr00t-n1_6/" target="_blank">research blog</a>]
			[<a href="https://github.com/NVIDIA/Isaac-GR00T" target="_blank">code</a>]
			[<a href="https://huggingface.co/nvidia/GR00T-N1.6-3B" target="_blank">huggingface</a>]
			</p>
		</td>
	</tr>
	
	<tr>
		<td width="306">
			<img src="pics/i-scene.gif" width="285" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</b><br>
			<br>Lu Ling, <b>Yunhao Ge</b>, Yichen Sheng, Aniket Bera<br>
					<em>arXiv:2206.09592, 2022.</em>
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2512.13683" target="_blank">paper</a>]
			[<a href="https://luling06.github.io/I-Scene-project/" target="_blank">project page</a>]
			[<a href="https://github.com/LuLing06/I-Scene-project" target="_blank">code</a>]
			[<a href="https://github.com/LuLing06/I-Scene-project?tab=readme-ov-file" target="_blank">huggingface</a>]
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/cosmos_2p5.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>Cosmos-Predict2.5 and Cosmos-Transfer2.5: Improved World Simulation with Video Foundation Models for Physical AI</b><br>
		<br>NVIDIA (<b>Yunhao Ge</b>: core contributor)<br>
		<p></p>
		<!-- <b>White Paper</b>. -->
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2511.00062" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/cosmos-predict2.5/" target="_blank">project page</a>]
			[<a href="https://github.com/nvidia-cosmos/cosmos-predict2.5" target="_blank">code</a>]
			[<a href="https://huggingface.co/collections/nvidia/cosmos-predict25" target="_blank">huggingface</a>]
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/cosmostransfer1.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>Cosmos-Transfer1: Conditional World Generation with Adaptive Multimodal Control</b><br>
			<br>NVIDIA (<b>Yunhao Ge</b>: core contributor)<br>
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2503.14492" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/cosmos-transfer1/" target="_blank">project page</a>]
			[<a href="https://github.com/nvidia-cosmos/cosmos-transfer1" target="_blank">code</a>]
			[<a href="https://huggingface.co/collections/nvidia/cosmos-transfer1-67c9d328196453be6e568d3e" target="_blank">huggingface</a>]
			[<a href="https://www.youtube.com/watch?v=0Yr5SdrVnxc" target="_blank">video</a>]
			</p>
		</td>
	</tr>


	<tr>
		<td width="306">
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/Cosmos.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>Cosmos: World Foundation Model Platform for Physical AI</b><br>
		<br>NVIDIA (<b>Yunhao Ge</b>: core contributor)<br>
		<p></p>
		<p><strong style="color:rgb(20, 115, 204)">Best AI + Best overall of CES 2025</strong></p>
		<!-- <b>White Paper</b>. -->
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2501.03575" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/cosmos1/" target="_blank">project page</a>]
			[<a href="https://github.com/NVIDIA/Cosmos" target="_blank">code</a>]
			[<a href="https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6" target="_blank">huggingface</a>]
			[<a href="https://www.youtube.com/watch?v=9Uch931cDx8" target="_blank">video</a>]
			[<a href="https://build.nvidia.com/nvidia/cosmos-predict1-7b" target="_blank">Demo API</a>]
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/DAM.mov" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>Describe Anything: Detailed Localized Image and Video Captioning</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
		Long Lian, Yifan Ding, <b>Yunhao Ge</b>, Sifei Liu, Hanzi Mao, Boyi Li, Marco Pavone, Ming-Yu Liu, Trevor Darrell, Adam Yala, Yin Cui<br>
		<b>ICCV 2025</b>.
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2504.16072" target="_blank">paper</a>]
			[<a href="https://github.com/NVlabs/describe-anything" target="_blank">code</a>]
			[<a href="https://describe-anything.github.io/" target="_blank">project page</a>]
			[<a href="https://huggingface.co/spaces/nvidia/describe-anything-model-demo" target="_blank">demo</a>]
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


	<tr>
		<td width="306">
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/edify-3d.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>Edify 3D: Scalable High-Quality 3D Asset Generation</b><br>
			<br>NVIDIA (<b>Yunhao Ge</b>: core contributor)<br>
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2411.07135" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/edify-3d/" target="_blank">project page</a>]
			[<a href="https://www.youtube.com/watch?v=ROqB8xhKZ6U" target="_blank">video</a>]
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>



	<tr>
		<td width="306">
			<img src="pics/edify-2d.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Edify Image: High-Quality Image Generation with Pixel Space Laplacian Diffusion Models</b><br>
			<br>NVIDIA (<b>Yunhao Ge</b>: core contributor)<br>
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2411.07126" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/edify-image/" target="_blank">project page</a>]
			[<a href="https://www.youtube.com/watch?v=opiaV94tMJ4&ab_channel=NVIDIADeveloper" target="_blank">video</a>]
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>



	<tr>
		<td width="306">
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/GenUSD.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>GenUSD: 3D scene generation made easy</b><br>
<br>
			Core contributor<br>
		<b>SIGGRAPH 2024</b>.
		<p></p>
			<p>[<a href="https://dl.acm.org/doi/fullHtml/10.1145/3641520.3665306" target="_blank">paper</a>]
			[<a href="https://blogs.nvidia.com/blog/real-time-3d-generative-ai-research-siggraph-2024/?ncid=so-twit-353134-vt36&linkId=100000277255797" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	
	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<img src="pics/bvs.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b><sup>*</sup>, Yihe Tang<sup>*</sup>, Jiashu Xu<sup>*</sup>, Cem Gokmen<sup>*</sup>, Chengshu Li, Wensi Ai, Benjamin Jose Martinez, Arman Aydin, Mona Anvari, Ayush K Chakravarthy, 
			Hong-Xing Yu, Josiah Wong, Sanjana Srivastava, Sharon Lee, Shengxin Zha, Laurent Itti, Yunzhu Li, Roberto Martín-Martín, Miao Liu, Pengchuan Zhang, 
			Ruohan Zhang, Li Fei-Fei, Jiajun Wu <br>
			(*=equal contribution)<br>
		<b>CVPR 2024</b> (<em>IEEE Conference on Computer Vision and Pattern Recognition</em>).
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2405.09546" target="_blank">paper</a>]
				[<a href="https://github.com/behavior-vision-suite/behavior-vision-suite.github.io" target="_blank">code</a>]
			<!-- [<a href="https://research.nvidia.com/labs/dir/vfc/assets/vfc_video.mp4" target="_blank">video</a>] -->
			[<a href="https://behavior-vision-suite.github.io/" target="_blank">project page</a>]
			[<a href="https://www.nvidia.com/en-us/omniverse/" target="_blank">tools</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>

			<p><strong style="color:rgb(20, 115, 204)">Highlight</strong></p>
		</td>
	</tr>
		
	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<img src="pics/vfc.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, Yin Cui <br>
		<b>CVPR 2024</b> (<em>IEEE Conference on Computer Vision and Pattern Recognition</em>).
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2404.19752" target="_blank">paper</a>]
			[<a href="https://research.nvidia.com/labs/dir/vfc/assets/vfc_video.mp4" target="_blank">video</a>]
			[<a href="https://research.nvidia.com/labs/dir/vfc/" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>
	
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	
	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<video width="285" autoplay loop muted playsinline style="box-shadow: 4px 4px 8px #888">
				<source src="pics/dreamdistribution.mp4" type="video/mp4">
				Your browser does not support the video tag.
			</video>
		</td>
		<td>
			<b>DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			Brian Nlong Zhao, Yuhang Xiao<sup>*</sup>, Jiashu Xu<sup>*</sup>, Xinyang Jiang, Yifan Yang, Dongsheng Li, Laurent Itti, Vibhav Vineet<sup>†</sup>, <b>Yunhao Ge</b><sup>†</sup>
			(*=co-second author, †=equal contribution)<br>
		<b>ICLR 2025</b>.
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2312.14216.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/briannlongzhao/DreamDistribution" target="_blank">code</a>]
			[<a href="https://briannlongzhao.github.io/DreamDistribution/" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>


	<tr>
		<td width="306">
<!--		<img src="pics/3dcopy.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
			<img src="pics/3D-copy-paste.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>3D Copy-Paste: Physically-Plausible Object Insertion for Monocular 3D Detection</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Hong-Xing Yu, Cheng Zhao, Yuliang Guo, Xinyu Huang, Liu Ren, Laurent Itti, Jiajun Wu <br>
		<b>NeurIPS 2023</b> (<em>Advances in Neural Information Processing Systems</em>).
		<p></p>
			<p>[<a href="https://arxiv.org/abs/2312.05277" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/3D-Copy-Paste" target="_blank">code</a>]
			[<a href="https://gyhandy.github.io/3D-Copy-Paste/" target="_blank">project page</a>]
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>

	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr>

	<tr>
		<td width="306">
		<img src="pics/dalle-for-detection.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>DALL-E for Detection: Language-driven Compositional Image Synthesis for Object Detection</b><br>
			<b>Beyond Generation: Harnessing Text to Image Models for Object Detection and Segmentation</b><br>
<br>
<!--		<br>-->
<!--			<b>TL;DR:</b> A new paradigm for automatic context image generation at scale with DALL-E (generative models) for downstream takes (discriminative models). <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b><sup>*</sup>, Jiashu Xu<sup>*</sup>, Brian Nlong Zhao, Neel Joshi, Laurent Itti, Vibhav Vineet (*=equal contribution) <br>
		<em>arXiv:2206.09592, 2022.</em>
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2309.05956.pdf" target="_blank">paper(Beyond Generation)</a>]
				[<a href="https://arxiv.org/pdf/2206.09592.pdf" target="_blank">paper(DALL-E for Detection)</a>]
			[<a href="https://github.com/gyhandy/Text2Image-for-Detection" target="_blank">code</a>]
<!--			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]-->
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			</p>
		</td>
	</tr>


	<!-- <tr>
		<td width="306">
		<img src="pics/channel-LR.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>CLR: Channel-wise Lightweight Reprogramming for Continual Learning</b> <br>
					<br> -->
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<!-- <b>Yunhao Ge</b>, Yuecheng Li<sup>*</sup>, Shuo Ni<sup>*</sup>, Jiaping Zhao, Ming-Hsuan Yang, Laurent Itti
			(*=equal contribution as second author) <br>
		<b>ICCV 2023</b> (<em>International Conference on Computer Vision</em>). -->
<!--		<b>NeurIPS 2022 Workshop</b> (<em>ML Safety Workshop </em>).-->
		<!-- <p></p>
			<p> -->

				<!-- [<a href="https://arxiv.org/pdf/2307.11386.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming" target="_blank">code</a>]
			[<a href="http://ilab.usc.edu/andy/skill" target="_blank">project page</a>]-->
			<!-- [<a href="http://ilab.usc.edu/andy/skill102" target="_blank">SKILL-102 Dataset</a>] --> 
<!--			[<a href="https://viterbischool.usc.edu/news/2023/07/teaching-robots-to-teach-other-robots/" target="_blank">USC Viterbi Press</a>]-->
<!--								[<a paper and code coming soon</a>]-->
			<!-- </p>
		</td>
	</tr> -->
	<!-- <tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> -->

	
	<!-- <tr>
		<td width="306">
		<img src="pics/skill2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		<img src="pics/SKILL-m.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Lightweight Learner for Shared Knowledge Lifelong Learning</b> <br>
					<br>
			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.
			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>
			<br>-->
			<!-- <b>Yunhao Ge</b>, Yuecheng Li<sup>*</sup>, Di Wu<sup>*</sup>, Ao Xu<sup>*</sup>, Adam M. Jones, Amanda Sofie Rios, Iordanis Fostiropoulos, Shixian wen, Po-Hsuan Huang, Zachary William Murdock, Gozde Sahin, Shuo Ni, Kiran Lekkala, Sumedh Anand Sontakke, Laurent Itti (*=equal contribution as second author) <br>
		<b>TMLR</b> (<em>Transactions on Machine Learning Research</em>). --> 
<!--		<b>NeurIPS 2022 Workshop</b> (<em>ML Safety Workshop </em>).-->
		<!-- <p></p>
			<p> -->

			<!-- [<a href="https://openreview.net/pdf?id=Jjl2c8kWUc" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Shared-Knowledge-Lifelong-Learning" target="_blank">code</a>]
			[<a href="http://ilab.usc.edu/andy/skill" target="_blank">project page</a>]
			[<a href="http://ilab.usc.edu/andy/skill102" target="_blank">SKILL-102 Dataset</a>]
			[<a href="https://viterbischool.usc.edu/news/2023/07/teaching-robots-to-teach-other-robots/" target="_blank">USC Viterbi Press</a>] -->
<!--								[<a paper and code coming soon</a>]-->
			<!-- </p>
		</td>


			 <tr>
		<td width="306">
		<img src="pics/one-class-2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Building One-class Detector for Anything: Open-vocabulary Zero-shot OOD Detection Using Text-image Models</b><br>
<br>
			<b>Yunhao Ge</b><sup>*</sup>, Jie Ren<sup>*</sup>, Jiaping Zhao, Kaifeng Chen, Andrew Gallagher,
		Laurent Itti, and Balaji Lakshminarayanan (*=equal contribution) <br>
		<em>Knowledge and Logical Reasoning workshop @ <b>ICML 2023</b> </em>
		<p></p>
			<p>[<a href="https://arxiv.org/pdf/2305.17207.pdf" target="_blank">paper</a>]
			</p>
		</td>
	</tr> -->



	<!-- <tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr> -->

	<tr>
		<td width="306">
		<img src="pics/multi-modal.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Improving Zero-shot Generalization and Robustness of Multi-modal Models</b> <br>
					<br>
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b><sup>*</sup>, Jie Ren<sup>*</sup>, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang,
		Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, and Jiaping Zhao (*=equal contribution) <br>

		<b>CVPR 2023</b> (<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition</em>).
<!--		<b>NeurIPS 2022 Workshop</b> (<em>ML Safety Workshop </em>).-->
		<p></p>
			<p>

				[<a href="https://arxiv.org/pdf/2212.01758.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Hierarchy-CLIP" target="_blank">code</a>]
				[<a href="https://sites.google.com/usc.edu/hierarchy-clip/" target="_blank">project page</a>]
<!--								[<a paper and code coming soon</a>]-->
			</p>
		</td>
	</tr>


<!--<table id="tbPublications" width="100%">-->

	<tr>
		<td width="306">
		<img src="pics/neural-sim.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Neural-Sim: Learning to Generate Training Data with NeRF</b> <br>
					<br>
<!--			<b>TL;DR:</b> The first fully differentiable synthetic data pipeline that uses NeRFs in a closed-loop with a target application's loss function.-->
<!--			Neural-Sim generates data on-demand, with no human labor, to maximize accuracy for a target task. <br>-->
<!--			<br>-->
			<b>Yunhao Ge</b>, Harkirat Behl<sup>*</sup>, Jiashu Xu<sup>*</sup>, Suriya Gunasekar, Neel Joshi,
		Yale Song, Xin Wang, Laurent Itti, and Vibhav Vineet (*=equal contribution as second author) <br>
		<b>ECCV 2022</b> (<em>European Conference on Computer Vision</em>).
		<p></p>
			<p>

								[<a href="https://arxiv.org/pdf/2207.11368.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Neural-Sim-NeRF" target="_blank">code</a>]
<!--								[<a paper and code coming soon</a>]-->
			</p>
		</td>
	</tr>



	<!-- <tr>
		<td width="306">
		<img src="pics/hve-2.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>Contributions of Shape, Texture, and Color in Visual Recognition</b><br> -->
<!--			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>-->
		<!-- <br> -->
<!--			<b>TL;DR:</b> Build a humanoid vision engine (HVE) that explicitly and separately computes shape, texture, and color features from images.-->
<!--			HVE can summarize and rank-order the contributions of the three features to object recognition. <br>-->
<!--			<br>-->
			<!-- <b>Yunhao Ge<sup>*</sup></b>, Yao Xiao<sup>*</sup>, Zhi Xu, Xingrui Wang, Laurent Itti (*=equal contribution) <br> -->
<!--		<em>European Conference on Computer Vision, </em>(<i><b>ECCV</b></i>), 2022.-->
			<!-- <b>ECCV 2022</b> (<em>European Conference on Computer Vision</em>).
		<p></p>
			<p>
			[<a href="https://arxiv.org/pdf/2207.09510.pdf" target="_blank">paper</a>]
				[<a href="https://github.com/gyhandy/Humanoid-Vision-Engine" target="_blank">code</a>] -->
<!--								[<a paper and code coming soon</a>]-->

<!--				[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" target="_blank">paper</a>]-->
<!--			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]-->
<!--			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]-->
<!--			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]-->
<!--			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]-->
<!--			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]-->
			<!-- </p>
		</td>
	</tr> -->

	<!-- <tr>
		<td width="306">
		<img src="pics/vrx-5.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td>
			<b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b><br> -->
<!--			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>-->
		<!-- <br> -->
<!--			<b>TL;DR:</b> Takes a step towards mimicking the reasoning process of NNs and provide logical, concept-level-->
<!--						explanations for final model decisions. <br>-->
<!--			<br>-->
			<!-- <b>Yunhao Ge</b>, Yao Xiao, Zhi Xu, Meng Zheng, Srikrishna Karanam, Terrence Chen, Laurent Itti and Ziyan Wu  <br> -->
<!--		<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition </em>(<i><b>CVPR</b></i>), 2021.-->
			<!-- <b>CVPR 2021</b> (<em>IEEE/ CVF International Conference on Computer Vision and Pattern Recognition</em>).
		<p></p>
			<p>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]
			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">project page</a>]
			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]
			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]
			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]
			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]
			</p>
		</td>
	</tr> -->




	<tr>
		<td width="306">
		<img src="pics/GSL3.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Zero-shot Synthesis with Group-Supervised Learning</b> <br>
		<br>
<!--		<b>TL;DR:</b> Using Controllable disentangled representation learning to simulate human knowledge factorization for imagination <br>-->
<!--			<br>-->
		<b>Yunhao Ge</b>, Sami Abu-El-Haija, Gan Xin and Laurent Itti  <br>
			<b>ICLR 2021</b> (<em>International Conference on Learning Representations</em>).
<!--		<em>International Conference on Learning Representations </em>(<i><b>ICLR</b></i>), 2021.-->
		<p></p>
		<p>[<a href="https://arxiv.org/pdf/2009.06586.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Group-Supervised-Learning" target="_blank">code</a>]
			[<a href="http://sami.haija.org/iclr21gsl" target="_blank">project page</a>]
			[<a href="http://ilab.usc.edu/datasets/fonts" target="_blank">Fonts Dataset</a>]
			[<a href="https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/" target="_blank">USC Viterbi Press</a>]
			[<a href="https://zhuanlan.zhihu.com/p/364895887" target="_blank">知乎</a>]
			[<a href="https://mp.weixin.qq.com/s/o2HBYf3NF3UsMxUqkASdyg" target="_blank">AI科技评论</a>]<br>
			[<a  target="_blank" href=https://viterbischool.usc.edu/news/2021/07/usc-researchers-enable-ai-to-use-its-imagination/ target="_blank" rel="external"> USC News </a>]
			[<a  target="_blank" href=https://techxplore.com/news/2021-07-enabling-artificial-intelligence.html target="_blank" rel="external"> Tech Xplore </a>]
			[<a  target="_blank" href=https://www.technologynetworks.com/informatics/news/enabling-ai-to-use-its-imagination-350886 target="_blank" rel="external"> Technology Networks </a>]
		</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>


		<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>



<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/ISL.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>Invariant Structure Learning for Better Generalization and Causal Explainability</b><br>-->
<!--&lt;!&ndash;			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>&ndash;&gt;-->
<!--		<br>-->
<!--&lt;!&ndash;			<b>TL;DR:</b> ISL splits the data into different environments, and learns a structure that is invariant to the target across different&ndash;&gt;-->
<!--&lt;!&ndash;				environments by imposing a consistency constraint. <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge</b>, Sercan Ö. Arik, Jinsung Yoon, Ao Xu, Laurent Itti and Tomas Pfister  <br>-->
<!--		<b>TMLR</b> (<em>Transactions on Machine Learning Research</em>)-->
<!--		<p></p>-->
<!--			<p>[<a href="https://arxiv.org/pdf/2206.06469.pdf" target="_blank">paper</a>]-->
<!--&lt;!&ndash;			[<a href="https://github.com/gyhandy/Visual-Reasoning-eXplanation" target="_blank">github</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="http://ilab.usc.edu/andy/vrx" target="_blank">website</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://youtu.be/ZzkpUrK-cRA" target="_blank">video</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://zhuanlan.zhihu.com/p/380264276" target="_blank">知乎</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/FhQsi7twHkGskcE5fshOxA" target="_blank">机器之心</a>]&ndash;&gt;-->
<!--&lt;!&ndash;			[<a href="https://mp.weixin.qq.com/s/DqYlLVMggNEWuN76-OXJmQ" target="_blank">AI科技评论</a>]&ndash;&gt;-->
<!--			</p>-->
<!--		</td>-->
<!--	</tr>-->


<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--	<tr>&nbsp</tr>-->

<!--			<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/CIR-1.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>Encouraging Disentangled and Convex Representation with Controllable Interpolation Regularization</b><br>-->
<!--&lt;!&ndash;			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>&ndash;&gt;-->
<!--		<br>-->
<!--&lt;!&ndash;		<b>TL;DR:</b> A simple yet efficient method: Controllable Interpolation Regularization (CIR), which creates a positive loop where the disentanglement and convexity can help each other  <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge</b>, Zhi Xu, Yao Xiao, Gan Xin, Yunkui Pang and Laurent Itti <br>-->
<!--		<b>WACV 2023</b> (<em>IEEE/CVF Winter Conference on Applications of Computer Vision</em>).-->
<!--		<p></p>-->
<!--		<p>[<a href="https://arxiv.org/pdf/2112.03163.pdf" target="_blank">paper</a>]</p>-->
<!--		</td>-->
<!--	</tr>-->

	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>
	<tr>&nbsp</tr>




<!--		<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/GraphAE.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td>-->
<!--			<b>Graph Autoencoder for Graph Compression and Representation Learning</b><br>-->
<!--&lt;!&ndash;			<p style="color:#2E86C1 ";> <b>A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts</b> </p><br>&ndash;&gt;-->
<!--		<br>-->
<!--&lt;!&ndash;		<b>TL;DR:</b> Multi-kernel Inductive Attention Graph Autoencoder (MIAGAE) utilizes the node similarity and graph structure to compress all nodes and edges as a whole.  <br>&ndash;&gt;-->
<!--&lt;!&ndash;			<br>&ndash;&gt;-->
<!--			<b>Yunhao Ge<sup>*</sup></b>, Yunkui Pang<sup>*</sup>, Linwei Li and Laurent Itti (*=equal contribution)<br>-->
<!--			<b>ICLR 2021 Workshop</b> (<em>Neural Compression: From Information Theory to Applications&#45;&#45;Workshop@ </em>).-->
<!--&lt;!&ndash;		<em>Neural Compression: From Information Theory to Applications&#45;&#45;Workshop@ </em>(<i><b>ICLR</b></i>), 2021.&ndash;&gt;-->
<!--		<p></p>-->
<!--		<p>[<a href="https://openreview.net/pdf?id=Bo2LZfaVHNi" target="_blank">paper</a>]-->
<!--			[<a href="https://github.com/Pangyk/Graph_AE" target="_blank">code</a>]-->
<!--			[<a href="http://ilab.usc.edu/datasets/i2sg" target="_blank">Img2SceneGraph</a>]</p>-->
<!--			<p><strong style="color:blue">Spotlight Presentation</strong></p>-->
<!--		</td>-->
<!--	</tr>-->


    <!-- <tr>
		<td width="306">
		<img src="pics/eccv.gif" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Pose Augmentation: Class-agnostic Object Pose Transformation for Object Recognition</b> <br>
			<br> -->
<!--			<b>TL;DR:</b> Eliminate-add generator to explicitly disentangle pose from object identity by maximizing pose entropy<br>-->
<!--			<br>-->
		<!-- <b>Yunhao Ge</b>, Jiaping Zhao, Laurent Itti  <br>
		<b>ECCV 2020</b> (<em>European Conference on Computer Vision</em>). -->
<!--		<em>European Conference on Computer Vision</em> (<i><b>ECCV</b></i>), 2020.-->
		<!-- <p></p>
		<p>[<a href="https://arxiv.org/pdf/2003.08526.pdf" target="_blank">paper</a>]
			[<a href="https://github.com/gyhandy/Pose-Augmentation" target="_blank">github</a>]
			[<a href="https://youtu.be/WHAFj9KXRFY" target="_blank">video-1min</a>]
			[<a href="https://youtu.be/9N8eyOmCWh4" target="_blank">video-10min</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr> -->

<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/Beneficial.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Beneficial Perturbation Network for designing general adaptive artificial intelligence systems</b> <br>-->
<!--&lt;!&ndash;		<b>TL;DR:</b>Allowing a single network to learn potentially unlimited parallel input to output mappings, and to switch on the fly&ndash;&gt;-->
<!--&lt;!&ndash;			between them at runtime.<br>&ndash;&gt;-->
<!--			<br>-->
<!--		Shixian Wen, Amanda Rios<sup>*</sup>, <b>Yunhao Ge<sup>*</sup></b> and Laurent Itti (*=equal contribution as second author) <br>-->
<!--			<b>TNNLS 2021</b> (<em> IEEE Transactions on Neural Networks and Learning Systems </em>).-->
<!--&lt;!&ndash;		<em> IEEE Transactions on Neural Networks and Learning Systems </em>(<i><b>TNNLS</b></i>), 2021.&ndash;&gt;-->
<!--		<p></p>-->
<!--		<p>[<a href="https://arxiv.org/pdf/2009.13954.pdf" target="_blank">paper</a>]-->
<!--		</td>-->




    <!-- <tr>
		<td width="306">
		<img src="pics/project11.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Unpaired MR to CT Synthesis with Explicit Structural Constrained Adversarial Learning</b> <br>
			 <br>
		<b>Yunhao Ge<sup>*</sup></b>, Dongming Wei<sup>*</sup>, Zhong Xue, Yiqiang Zhan, Xiang Zhou, Qian Wang and Shu Liao (*=equal contribution)<br>
			<b>ISBI 2019</b> (<em>IEEE International Symposium on Biomedical Imaging</em>).
		<p></p>
		<p>[<a href="files/UNPAIRED.pdf" target="_blank">paper</a>]
            [<a href="https://github.com/gyhandy/Unpaired-Cross-modality-Image-Synthesis" target="_blank">code</a>]</p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr> -->


<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/SLIR-MedIA.jpg" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Synthesis and inpainting-based MR-CT registration for image-guided thermal ablation of liver tumors</b> <br>-->
<!--			<br>-->
<!--		Dongming Wei, Sahar Ahmad, Jiayu Huo, Wen Peng, <b>Yunhao Ge</b>, Zhong Xue, Pew-Thian Yap, Wentao Li, Dinggang Shen, Qian Wang <br>-->
<!--			<b>MICCAI 2019</b> (<em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em>).-->
<!--&lt;!&ndash;		<em>International Conference on Medical Image Computing and Computer-Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2019.&ndash;&gt;-->
<!--		<p></p>-->
<!--		<p>[<a href="https://arxiv.org/pdf/1907.13020.pdf" target="_blank">paper</a>]</p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

    <!-- <tr>
		<td width="306">
		<img src="pics/SPIE1.png" width="285px" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td> <b>Unpaired Whole-body MR to CT Synthesis with Correlation Coefficient Constrained Adversarial Learning</b> <br>
			<br>
		<b>Yunhao Ge</b>, Zhong Xue, Yiqiang Zhan, Xiang Zhou and Shu Liao <br>
		<b>SPIE 2019</b> (<em>SPIE-Medical Imaging</em>).
		<p></p>
		<p>[<a href="files/SPIE.pdf" target="_blank">paper</a>]
            [<a href="https://github.com/gyhandy/Unpaired-Cross-modality-Image-Synthesis" target="_blank">code</a>]</p>

		<p><strong style="color:blue">Oral Presentation</strong></p>
		</td>
	</tr>
	<tr>&nbsp</tr>
    <tr>&nbsp</tr>
    <tr>&nbsp</tr> -->


<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project3.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>A Real-time Gesture Prediction System Using Neural Networks and Multimodal Fusion-->
<!--					based on Data Glove</b> <br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>IEEE International Conference on Advanced Computational Intelligence</em> (<i><b>ICACI</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Real.pdf" target="_blank">paper</a>]</p>-->
<!--			<p><strong style="color:blue">Oral Presentation</strong></p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/HHnet.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>HH-Net: Image driven microscope fast auto-focus with deep neural network</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li, Yanzheng Zhao and Weixin Yan <br>-->
<!--		<em>International Conference on Biomedical Engineering and Technology</em> (<i><b>ICBET</b></i>), 2019.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/HH-net.pdf" target="_blank">paper</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project4.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Melanoma Segmentation and Classification in Clinical Images Using Deep Learning</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Bin Li and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Melanoma.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195164">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project5.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Benign and Malignant Mammographic Image Classification based on Convolutional-->
<!--					Neural Networks</b><br>-->
<!--			<br>-->
<!--		Bin Li, <b>Yunhao Ge</b>, Yanzheng Zhao, Enguang Guan and Weixin Yan <br>-->
<!--		<em>ACM International Conference on Machine Learning and Computing</em> (<i><b>ICMLC</b></i>), 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Benign.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://dl.acm.org/citation.cfm?id=3195163&dl=ACM&coll=DL">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Effect of Mechanical Error on Dual-Wedge Laser Scanning System and Error Correction</b><br>-->
<!--			<br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Fenfen Xue, Enguang Guan, Weixin Yan and Yanzheng Zhao <br>-->
<!--		<i><b>Applied Optics</b></i>, 2018.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Effect.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://www.osapublishing.org/ao/abstract.cfm?uri=ao-57-21-6047">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->

<!--    <tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project8.png" width="285px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td> <b>Dynamic Drive Performances of the Bionic Suction Cup Actuator Based on Shape Memory Alloy</b><br>-->
<!--		<b>Yunhao Ge</b>, Jihao Liu, Bin Li, Weixin Yan and Yanzheng Zhao  <br>-->
<!--		<em>Intelligent Robotics and Applications</em> (<i><b>ICIRA</b></i>), 2017.-->
<!--		<p></p>-->
<!--		<p>[<a href="files/Dynamic.pdf" target="_blank">paper</a>][<a  target="_blank" href="https://link.springer.com/chapter/10.1007%2F978-3-319-65289-4_2">Paper Link</a>]-->
<!--        </p>-->
<!--		</td>-->
<!--	</tr>-->
<!--	<tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->
<!--    <tr>&nbsp</tr>-->


<!--	<tr>-->
<!--		<td width="306">-->
<!--		<img src="pics/project7.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--		</td>-->
<!--		<td><strong>Interactive Experience across Modern Age and Tradition: Application of Arduino in Shadow Play</strong><br>-->
<!--            <em>Kun Yin, <b>Yunhao Ge</b>, Heshan Liu and Yongquan Yin  <br></em>-->
<!--            <em>Advances in Mechatronics and Machinery </em>.-->
<!--			<br>[<a target="_blank" href="files/Interactive.pdf">PDF</a>] [<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]-->
<!--&lt;!&ndash;		<br>[<a  target="_blank" href="https://www.scientific.net/AMM.868.242">Paper Link</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--		</td>-->
<!--	</tr>-->



</table>
<br>
<h2><a id="C4" ><font color="#CB4335">Intern & Work Experience</font></a></h2>

      <h4> Google Research, Los Angeles, USA  (May. 2022 - Dec. 2022) </h4>
	  <ul>
        <li>Position: Student Researcher in <a  target="_blank" href=https://research.google/ target="_blank" rel="external"> Google Research </a></li>


		  <li>Supervisor: Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=j64_S3EAAAAJ&hl=en rel="external"> Jiaping Zhao </a>,
			  Dr. <a  target="_blank" href=https://jessieren.github.io/ target="_blank" rel="external"> Jie Ren </a>,
			  Dr. <a  target="_blank" href=http://www.gatsby.ucl.ac.uk/~balaji/ target="_blank" rel="external"> Balaji Lakshminarayanan </a>
        and Prof. <a  target="_blank" href=http://faculty.ucmerced.edu/mhyang/ target="_blank" rel="external"> Ming-Hsuan Yang</a></li>
        <li>Project: Improving Zero-shot Generalization and Robustness of Multi-modal Models --> <b>Accepted by CVPR 2023</b> </li>
    </ul>

      <h4> Google Cloud AI, Mountain View, USA  (Aug. 2021 - May 2022) </h4>
	  <ul>
        <li>Position: Student Researcher in <a  target="_blank" href="https://cloud.google.com/" target="_blank" rel="external">Google Cloud AI</a></li>
		  <li>Supervisor: Dr. <a  target="_blank" href=https://sercanarik.com/ rel="external"> Sercan Arik </a>
        and Dr. <a  target="_blank" href=https://sites.google.com/view/jinsungyoon target="_blank" rel="external"> Jinsung Yoon </a></li>
        <li>Project: Causal Explainable AI --> <b>Accepted by TMLR</b> </li>
    </ul>

      <h4> Microsoft Research, Redmond, USA  (May. 2021 - Aug. 2021) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.microsoft.com/en-us/research/research-area/computer-vision/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13562&sort_by=most-recent" target="_blank" rel="external">Microsoft Research Computer Vision Group</a></li>
		  <li>Supervisor: Dr. <a  target="_blank" href=http://vibhavvineet.info/ target="_blank" rel="external"> Vibhav Vineet </a>
            and Dr. <a  target="_blank" href=https://neelj.com/ target="_blank" rel="external"> Neel Joshi </a></li>
        <li>Project: On-demand training data generation with Neural Rendering --> <b>Accepted by ECCV 2022</b> </li>
    </ul>

<!--    <img id="school_logo" src="./pics/uII.png">-->
      <h4> UII America, Inc, Boston, USA  (May. 2020 - Aug. 2020) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">UII America</a></li>
        <li>Supervisor: Dr. <a  target="_blank" href=http://wuziyan.com/ target="_blank" rel="external"> Ziyan Wu </a>
            and Dr. <a  target="_blank" href=https://karanams.github.io/ target="_blank" rel="external"> Srikrishna Karanam </a></li>
        <li>Project: Explain reasoning logic of NN --> <b>Accepted by CVPR 2021</b> </li>
    </ul>

<!--    <img id="school_logo" src="./pics/Flexiv.png">-->
          <!--<h4> The University of North Carolina at Chapel Hill, NC, USA  & <br> Shanghai United ImagingIntelligence Co., Ltd, China  (Jun. 2018 - Nov. 2018)</br> </h4>-->
          <h4> Flexiv Ltd, Shanghai, China  (May. 2019 - Aug. 2019) </h4>
          <ul>
            <li>Position: Computer Vision Engineer in <a  target="_blank" href="http://flexiv.com/" target="_blank" rel="external">Flexiv Robotics</a></li>
            <li>Supervisor: Prof.<a  target="_blank" href=http://webcache.googleusercontent.com/search?q=cache:http://mvig.sjtu.edu.cn/ target="_blank" rel="external"> Cewu Lu</a> and
				Dr. <a  target="_blank" href=https://www.linkedin.com/in/shuyun-chung-67b5337a/ target="_blank" rel="external"> Shuyun Chong</a>
			</li>
            <li>Project: Robotics adaptive massage based on human pose detection and tracking </li>
            <br/>
            <br/>
            <td width="306">
            <img src="pics/masage.gif" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">
            </td>
      </ul>

<!--    <img id="school_logo" src="./pics/uII.png">-->
      <h4> United Imaging Intelligence Co., Ltd, Shanghai, China  (Jun. 2018 - Apr. 2019) </h4>
	  <ul>
        <li>Position: Research Intern in <a  target="_blank" href="https://www.uii-ai.com/en/" target="_blank" rel="external">United Imaging Intelligence</a></li>
        <li>Supervisor: Prof.<a  target="_blank" href=https://scholar.google.com/citations?user=v6VYQC8AAAAJ&hl=zh-CN target="_blank" rel="external"> Dinggang Shen</a>
        and Dr. <a  target="_blank" href=https://scholar.google.com/citations?user=INL-unYAAAAJ&hl=en target="_blank" rel="external"> Shu Liao </a></li>
        <li>Project: Unpaired MR to CT image synthesis --> <b>Accepted by ISBI 2019 and MICCAI 2019</b>  </li>
    </ul>


<br>

<h2><font color="#CB4335">Scholarships</font></h2>
      <ul>
		<li>
          Aug. 2022 <b>Amazon ML Fellowship (2022-2023)</b></li>
		<li>
			Apr. 2022 <b>Annenberg Project Grant for simulating human imagination</b>, awarded annually to 10 PhD students across USC
			for high impact projects</li>
		<li>
          Aug. 2019 <b>Annenberg Graduate Fellowship at University of Southern California</b></li>
        <li>
          Sep. 2017 <b>National Scholarship(Graduate)</b>, (highest honor for graduates) <strong style="color:blue">top 1% nationwide</strong></li>
        <li>
          Sep. 2015 <b>National Scholarship(Undergraduate)</b>, (highest honor for undergraduates, top 2% nationwide) </li>
		<li>
          May. 2018 <b>KaiYuan Motivational Scholarship</b> <strong style="color:blue">top 0.5% in Shanghai Jiao Tong University</strong></li>
        <li>
          Sep. 2015 <b>Presidential Scholarship</b>, (highest honor in Shandong University) <strong style="color:blue">top 0.2% in Shandong University</strong></li>
        <li>
          Sep. 2015 <b>BaoGang Excellent student Scholarship</b>, (4 Places per year at Shandong University) </li>
		<li>
          Sep. 2015 &amp; Sep 2014 &amp; Sep 2015</b> <b>First Prize Scholarship</b> (Top 6% in China,three-year continuous)</li>
      </ul>

<br>
<!--<h2><font color="#CB4335">Honors and Awards</font></h2>-->
<!--      <ul>-->
<!--        <li>-->
<!--          Aug. 2017 <b>The First Prize</b> 2017 ROBOMASTER <strong style="color:blue">The World’s Leading Robotics Competition</strong> (Responsible for the design-->
<!--of electronic control in robotics)[<a  target="_blank" href="https://github.com/gyhandy/Andy">Code-T_Infantry</a>]</li>-->
<!--		<li>-->
<!--          Aug. 2017 <b>Rank 1st</b> (preliminary competition) , Tianchi: Precision medical competition-Artificial Intelligence Aided-->
<!--genetic risk prediction of diabetes [<a  target="_blank" href="https://github.com/gyhandy/Diabetes-Mellitus">Code-Pred_diabetes</a>]</li>-->
<!--        <li>-->
<!--          Oct. 2015 <b>The First Prize</b> 9th international college students ican innovation and entrepreneurship contest</li>-->
<!--        <li>-->
<!--          Oct. 2015 <b>The Special prize(Top 1%)</b> 6th National University Students Process Equipment Practice and Innovation Competition</li>-->
<!--		<li>-->
<!--          Aug. 2015 <b>The Silver prize</b> 8th national college students in energy saving social practice and science and technology competition </li>-->
<!--      </ul>-->

<!--<br>-->


<h2><font color="#CB4335">Academic Service</font></h2>
Reviewer of the following conferences/journals:
<br>
<br>
NeurIPS 2023, 2022, 2021<br>
CVPR 2023, 2022<br>
ECCV 2022<br>
ICCV 2023, 2021<br>
ICLR 2023, 2022<br>
ICML 2022<br>
WACV 2023<br>


<!--2021 IEEE CVPR WORKSHOP ON FAIR, DATA EFFICIENT AND TRUSTED COMPUTER VISION<br>-->
IEEE Transactions on Medical Imaging (TMI)<br>
IEEE Access<br>
Applied Optics<br>

<br>





<!--  <h2><font color="#CB4335">Software & Patents</font></h2>-->
<!--<table id="tbPublications" width="100%">-->


<!--	<tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/uspatent.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>Systems and methods for image processing</strong><br>-->
<!--            <em> S Liao, <b>GE Yunhao</b>, WEI Dongming<br></em>-->
<!--            <em>US Patent App. 16/729,303</em>.-->
<!--&lt;!&ndash;        <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/software.jpg" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>Pulmonary Nodular Assisted Detection System Based on AI(V1.0)</strong><br>-->
<!--            <em> Bin Li, <b>Yunhao Ge</b><br></em>-->
<!--            <em>2018SR037095</em>.-->
<!--&lt;!&ndash;        <br>[<a href="https://ieeexplore.ieee.org/abstract/document/8413124/">Paper</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/patent1.jpg" width="285px" height= "170px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>A Two-Layer Barrier Free Parking Equipment Based on Bionic Manipulator</strong><br>-->
<!--            <em><b>Yunhao Ge</b>, Shangze Yang, Zheng Zhang, Weixin Yan and Yanzheng Zhao <br></em>-->
<!--            <em>CN201610712048</em>.-->
<!--&lt;!&ndash;        <br>[<a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w3/Xu_Dual-Mode_Vehicle_Motion_CVPR_2018_paper.pdf">Paper</a>]&ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->

<!--    <tr>-->
<!--        <td width="306">-->
<!--        <img src="pics/patent2.png" width="285px" height= "180px" style="box-shadow: 4px 4px 8px #888">-->
<!--        </td>-->
<!--        <td><strong>A Double Decker Parking Equipment based on Shear Lifting Mechanism and Hydraulic Mechanism</strong><br>-->
<!--            <em><b>Yunhao Ge</b>, Xulong Zhou, Peng Liu and Yanzheng Zhao <br></em>-->
<!--            <em>CN201610704408</em>.-->
<!--&lt;!&ndash;        <br>[<a href="https://github.com/gyhandy/publication/blob/master/A%20Real-time%20Gesture%20Prediction%20System%20Using%20Neural%20Networks%20and%20Multimodal%20Fusion%20based%20on%20data%20glove.pdf">Paper</a>] &ndash;&gt;-->
<!--        <br><br>-->
<!--        </td>-->
<!--    </tr>-->
<!--</table>-->
<!--<br>-->
<!--
 <h3>Academice Service</h3>
<hr>
<table id="tbActivities" border="0" width="100%">
	Reviewer of the following conferences/journals:
    <p>IEEE Transactions on Multimedia (TMM) </p>
    <p>IEEE International Conference on Computer Vision (ICCV 2017)</p>
    <p>ACM Multimedia Conference (MM 2017)</p>
    <p>IEEE International Conference on Image Processing (ICIP 2017)</p>
    <p>AAAI Conference on Artificial Intelligence (AAAI 2016)</p>
</table> 
-->

<!-- <h2>OpenCourse Achievements</h2>-->
<!--      <h4> DeepLearning.ai</h4>-->
<!--      <ul>-->
<!--        <li>Neural Networks and Deep Learning</li>-->
<!--        <li>Improving Deep Neural Networks</li>-->
<!--        <li>Structuring Machine Learning Projects </li>-->
<!--		<li>Convolutional Neural Networks </li>-->
<!--  </ul> -->
<!--<br> -->
<!--
<h4>Links</h4>
<strong>
<a href="http://cs231n.stanford.edu/">CS231n@Stanford</a><br>
<a href="http://pytorch.org/">PyTorch</a><br>

</strong>
-->
<hr>

<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=n&d=F_s9W3nU5OYqMT1Q8FiSEnLSHBjVFBhCTQoaSexG7Mk'></script>
<p align="center"><font color="#999999">Last update: December 29, 2025</font></p>



</body>

</html>
